# Speech_Emotion_Recognition_Using_LTSM.ipynb
This is my project scientific research. I had used filter MFCC for extract featuer and used LSTM model for train data.<br>
My model:<br>
![image](https://user-images.githubusercontent.com/86609606/191217118-38d7a007-f7aa-41a5-b963-b75b0bfb0a3b.png)<br>
My result:<br>
+ Accuracy:<br>
![image](https://user-images.githubusercontent.com/86609606/191216238-d0514f60-ccb5-4f8e-add7-094be9c67757.png)
+ Loss:<br>
![image](https://user-images.githubusercontent.com/86609606/191216590-347e6816-0e5f-4332-8050-5f7c8fee998a.png)
+ Evaluate train:<br>
![image](https://user-images.githubusercontent.com/86609606/191217272-1c83fab9-14e3-471a-a78a-abbfa817346c.png)
+ Evaluate test:<br>
![image](https://user-images.githubusercontent.com/86609606/191216831-b3bc2628-ef1c-4dd2-853e-4a37a138a00c.png)<br>
Link dataset my used: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess 
More dataset:
+https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio
+https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee
+https://www.kaggle.com/datasets/ejlok1/cremad

